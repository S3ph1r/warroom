"""
Load Universal Transactions to DB

Script to load the CSV generated by universal_pdf_parser.py into the PostgreSQL database.
Handles:
- Data type conversion
- Operation mapping (Acquisto -> BUY, etc)
- Deduplication (optional)
"""
import pandas as pd
import sys
import os
from sqlalchemy import create_engine, text
from datetime import datetime
from pathlib import Path

# Adjust path to import backend modules
# Hardcoded root for reliability in this specific environment
PROJECT_ROOT = r"d:\Download\Progetto WAR ROOM\warroom"
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

try:
    from backend.database import get_db_url
except ImportError:
    # Fallback for relative run
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from backend.database import get_db_url

DB_URL = get_db_url()
CSV_PATH = "data/extracted/BG_SAXO_Transactions_FinalVersion.csv"

def load_data():
    if not os.path.exists(CSV_PATH):
        print(f"‚ùå File not found: {CSV_PATH}")
        return

    print(f"üìÇ Loading: {CSV_PATH}")
    df = pd.read_csv(CSV_PATH)
    
    # Map Operations to DB Enums
    # DB Enums: BUY, SELL, DEPOSIT, WITHDRAW, DIVIDEND, FEE, TAX
    op_map = {
        'BUY': 'BUY',
        'Acquista': 'BUY',
        'AUTO_BUY': 'BUY', # Treat auto-reinvest as buy
        
        'SELL': 'SELL',
        'Vendi': 'SELL',
        'AUTO_SELL': 'SELL', # Treat auto-sell as sell
        
        'DEPOSIT': 'DEPOSIT',
        'Deposito': 'DEPOSIT',
        
        'WITHDRAW': 'WITHDRAW',
        'Prelievo': 'WITHDRAW',
        
        'DIVIDEND': 'DIVIDEND',
        'Div': 'DIVIDEND',
        
        'CORP_ACTION': 'DIVIDEND', # Default fallback or treat as needed
        'FEE': 'FEE'
    }
    
    # Apply Mapping
    df['operation'] = df['type'].map(op_map).fillna('UNKNOWN')
    
    # Check for unmapped types
    unknowns = df[df['operation'] == 'UNKNOWN']['type'].unique()
    if len(unknowns) > 0:
        print(f"‚ö†Ô∏è Warning: Unmapped types found: {unknowns}")
    
    # Map Columns to DB Schema
    # DB Table: transactions (date, symbol, quantity, price, currency, operation, user_id, source)
    engine = create_engine(DB_URL)
    
    with engine.begin() as conn:
        # Optional: Clear old generic imports
        print("üßπ Cleaning old 'pdf_import' data...")
        conn.execute(text("DELETE FROM transactions WHERE source = 'universal_pdf_parser'"))
        
        # Broker Logic: Currently hardcoded for BG SAXO module
        # In a generic pipeline, this would come from the directory name or file metadata
        TARGET_BROKER = "BG SAXO"
        
        print(f"üöÄ Inserting {len(df)} rows for broker: {TARGET_BROKER}...")
        
        for _, row in df.iterrows():
            # Construct insert dict
            # Use ISIN as symbol if available, else raw_name
            symbol = row['isin'] if pd.notna(row['isin']) else row['raw_name']
            
            conn.execute(
                text("""
                    INSERT INTO transactions 
                    (date, symbol, quantity, price, currency, operation, user_id, source, notes, portfolio_id)
                    VALUES (:date, :symbol, :quantity, :price, :currency, :operation, :user_id, :source, :notes, :portfolio_id)
                """),
                {
                    "date": pd.to_datetime(row['date']).date(),
                    "symbol": symbol[:50], 
                    "quantity": float(row['quantity']),
                    "price": float(row['price']),
                    "currency": row['currency'],
                    "operation": row['operation'],
                    "user_id": 1, 
                    "source": "universal_pdf_parser",
                    "notes": f"Broker: {TARGET_BROKER} | Original: {row['raw_name']} | Type: {row['type']}",
                    "portfolio_id": 1 # Default portfolio, should be mapped to Broker
                }
            )
            
    print("‚úÖ Data successfully loaded into DB with Broker attribution!")

if __name__ == "__main__":
    load_data()
